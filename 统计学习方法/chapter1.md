### 第一章 概述

统计学习：基于数据构建概率统计模型，并运用模型对数据进行预测和分析从训练数据集出发；

假设要学习的模型属于某个函数的集合，称为假设空间；应用某个评价准则，并根据准则设计算法，从假设空间中选取最优的模型。

监督学习的任务是根据训练数据生成一个输入到输出的预测模型，即找出输入变量X和输出变量Y之间的关系。

输入空间、特征空间、输出空间。输入、特征、输出通常都是向量，而对应的空间是欧氏空间。

输入空间和特征空间有时相同，有时需要做一个映射（可以理解为提取有意义的特征）。

模型通常定义在特征空间上。模型属于输入空间到输出空间的映射的集合，这个集合就是假设空间，假设空间的确定意味着学习范围的确定。

#### 问题类型

X和Y均为连续：回归问题；

Y为有限个离散变量：分类问题；

X和Y均为变量序列：标注问题。

回归举例：根据季节变化、行业产能调整等因素预测某地区下季度的电力需求。

分类举例：根据网页文本判断属于（政治、经济、体育）等哪一类主题。

标注举例：自然语言处理中的词性标注，例如一个英文句子，判断每一个单词属于主语、谓语、宾语......

**回归常用方法**：

线性回归，多项式回归，神经网络

**分类常用方法**：

大部分统计学习模型：感知机、神经网络、朴素贝叶斯、决策树、svm等

**标注常用方法**：

隐马尔可夫模型、条件随机场

X和Y遵循联合概率分布P(X,Y)是监督学习的基本假设。它的基本定义对于系统来说是未知的，训练数据和测试数据独立同分布。

#### 三要素

**模型(定义输入和输出之间如何联系)**

具体的，模型的形式可以理解为条件概率分布P(Y|X)和决策函数Y=f(X)两种。那么假设空间就是他们的集合（函数族）：或者，其中取值于n维欧氏空间（即函数中有n个需要确定的参数）。如果模型的预测能力好，那么训练样本和f()之间的差就应该足够小。学习系统就是不断尝试（调整参数）在假设空间里找出最好的模型（在测试集上所有的样本点表现优良，并且尽量在未知的数据上也能有较好的推广【泛化】。）

**策略（如何选择最准确的模型）**

损失函数：模型一次预测的好坏

风险函数：平均意义下模型预测的好坏

经验风险：在样本集上的平均损失（样本容量趋于无穷时趋于期望风险）。

结构风险：增加了正则惩罚项的风险定义，防止过拟合。

从中选择最优的f作为决策函数。用损失函数度量预测值f(X)和真实值Y比较的错误程度。计作。常用的损失函数：0-1，平方，绝对，对数。（其实如果从抽象意义上“距离”的角度来看，这些都是不同的“距离”）

损失越小，模型越好。

经验风险：

结构风险：

结构风险增加了定义在假设空间上的泛函和参数。通常和模型复杂度成正比，表示对模型复杂度的制约（在一般的观念里，模型越复杂意味着泛化能力越弱，我认为这一点并不一定是正确的，在一些论文中也有类似观点佐证。一种说法是，与数据复杂度相匹配的模型复杂度才是好的），而则代表制约程度。那么最后的问题就变成了最小化经验风险或者结构风险（最优化问题）。

**算法（如何最小化风险）**

在机器学习的领域，通常不存在解析解，需要用数值计算的方法。

像最小二乘法，梯度下降法等等都属于此列。



#### 泛化和过拟合（泛化就是对未知数据的预测能力，过拟合就是泛化得不好）

过拟合：模型对已知数据预测很好，对未知数据预测很差的现象。通过模型的泛化误差上界来分析模型的泛化能力：

定理：当假设空间是时，对任意f，至少以概率，以下不等式成立：;

其中定理可通过Hoeffding不等式证明。

这个式子表明，样本数N越大，泛化误差越小，（有限）假设空间容量越大，泛化误差越大。

对于无限假设空间的情况，书中没有说明，只能认为有限的情况是无限情况的一种启发。



#### 生成模型vs判别模型

生成模型采用的方法是先学习联合概率分布P(X|Y),然后求出条件概率分布P(Y|X)。并基于该条件概率分布做预测。

具体的式子是。之所以叫生成模型，是因为模型表示了给定输入X产生输出Y的生成关系。典型的生成模型有：朴素贝叶斯，隐马尔可夫模型......

而判别模型是直接求出或f(x)。大部分模型都是判别模型，如决策树、logistic回归，支持向量机等等。

生成方法优点：学习收敛更快；适用于存在隐变量的情况；
判别方法优点：因为直接学习目标，往往准确率更高；可以对数据进行各种程度上的抽象，更适合用特征提取等方法简化学习过程。https://www.zhihu.com/question/20446337