自然语言问题，有很多并不只是通过字面就能回答的。

例如：

已知：大象比老虎大，老虎比豹子大。

问题：大象跟豹子谁大？

对于人来说，这个问题很好回答：大象。

但是，对于机器来说，如果只是看文字，很难回答这个问题，因为它无法明白大小这种关系是可以递推的。

我们暂且不考虑形式化的逻辑推理是怎样的，先来考虑小朋友们是如何学会这种递推关系的。

他们在日常生活中，拿起两个东西，先是在大人的帮助下明白了大小关系。接着，时常能看到大、中、小的东西在生活的各个角落，也看到了他们之间大小关系的递进。从而学会了这种关系上的递推。

对于谁轻谁重这种问题来说也是一样。

可是对于机器来说，没有这种经历啊，如果不借助视觉的帮助，想要仅仅通过自然语言的方式学会这种关系，有可能吗？



我认为答案是肯定的，如果我们提供大量的语料，来表达这种关系，模型可以学到这种关系：

例如很多诸如下列形式的语料：

input：A比B大，B比C大

output：A比C大

训练一个encoder-decoder结构的生成模型，根据上述input生成output，那么，我们可以说这个生成模型已经理解了大小关系的可递推性。



再比如，我们是如何通过视觉上的观察，推断两个人谁轻谁重呢，按道理，我们视觉上只能看出来“谁比谁体积大”。但是，我们通过“体积大的物体通常比体积小的物体重”这一常识，作出了如上判断。

这样看来，有的“常识”是典型的概率推理，有的“常识”是典型的严格逻辑推理。而且，严格逻辑推理是可以划归到概率推理的范畴的——逻辑是置信度很高很高，接近于1的推理。



所以，如果我们用文字的形式描述各种各样的常识，让机器去学习“A导致B”，“因为A，所以B”，“A和B冲突”，“A和B无关“……等等各式各样的”常识“。

达成的效果是，一个生成模型根据输入（例如A），就会说”B可能会发生”，或者“B不可能发生”……，那么，是不是这个模型（看起来）就具有了一定的“常识”呢。



上面提到了这个模型可以是一种常见的生成模型，例如encoder-decoder架构。那么，这个模型和以前用于对话、翻译的seq2seq模型又有什么区别呢？

事实上，从模型上来说，和bert所用的transformer并无太大区别，区别仅在于数据。

bert的语料是通用的，所以，它只是通过建模“上句下面可能出现什么样的下句”来进行训练。并不会区分“上下句之间有逻辑关系”和“上下句之间只是松散的铺陈关系”……所以，它没有偏向于关系推理。我认为，这也是它虽然效果拔群，但是还是被诟病推理能力弱，更多只是基于相似度的语言表示方案的原因。



一些common sense相关的资料：

https://mosaic.allenai.org/publications

https://arxiv.org/abs/1906.02361

